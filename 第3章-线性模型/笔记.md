# 第3章 线性模型

## 基本形式
线性模型：试图学得一个通过属性的线性组合来进行预测的函数，即
$$f(x) = w_1x_1 + w_2x_2 + ... + w_dx_d + b$$

<br>

## 线性回归
首先我们看一个属性只有一个的例子：

线性回归试图学得
$$f(x_i) = wx_i + b，使得 f(x_i) \approx y_i$$
此时可以通过使均方误差 (MSE) 最小化来求解 $w$ 和 $b$。均方误差就是测量值的平方和的均值，即
$$(w^*, b^*) = \argmin_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2$$
基于均方误差最小化来进行模型求解的方法称为 “最小二乘法”

因此，可以直接将上式分别对 $w$ 和 $b$ 求导，并令其等于零，得到 $w$ 和 $b$ 最优解的闭式解
>闭式解：即解析解，指由严格的公式求出来的解，对于任意独立变量，我们都可以将其代入解析式，求得正确的值<br><br>数值解：和解析解对应，是指给出一系列对应的自变量，采用数值方法求出的解。他人只能利用数值计算的结果，而不能随意给出自变量并求出计算值<br><br>通俗理解：已知授人以鱼不如授人以渔，则解析解=渔，数值解=鱼

$$\begin{aligned}
\frac{\partial E_{(w,b)}}{\partial w}&=\frac{\partial ((y_1-wx_1-b)^2 + ... + (y_m-wx_m-b)^2)}{\partial w} \\ 
&=2(y_1-wx_1-b)(-x_1) + ... + 2(y_m-wx_m-b)(-x_m) \\
&=2(w\sum_{i=1}^mx_i^2 - \sum_{i=1}^m(y_i-b)x_i), \\

\frac{\partial E_{(w,b)}}{\partial b}&=\frac{\partial ((y_1-wx_1-b)^2 + ... + (y_m-wx_m-b)^2)}{\partial b} \\
&=2(mb - \sum_{i=1}^m(y_i-w_xi))
\end{aligned}$$

令其等于零，解得

$$\begin{aligned}
b&=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i), \\
w\sum_{i=1}^m{x_i^2}&=\sum_{i=1}^m(y_i-b)x_i \\
&=\sum_{i=1}^m{y_ix_i} - \sum_{i=1}^m{bx_i},
\end{aligned}$$

由于
$$\begin{aligned}
b&=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i) \\
&=\overline{y}-w\overline{x},
\end{aligned}$$
代入得
$$\begin{aligned}
w\sum_{i=1}^m{x_i^2}&=\sum_{i=1}^m{y_ix_i} - \sum_{i=1}^m{(\overline{y}-w\overline{x})x_i} \\
&=\sum_{i=1}^m{y_ix_i} - \overline{y}\sum_{i=1}^m{x_i} + w\overline{x}\sum_{i=1}^m{x_i},
\end{aligned}$$
因此有
$$\begin{aligned}
w&=\frac{\sum_{i=1}^my_ix_i - \overline{y}\sum_{i=1}^m{x_i}}{\sum_{i=1}^m{x_i^2} - \overline{x}\sum_{i=1}^m{x_i}} \\
&=\frac{\sum_{i=1}^m{y_ix_i} - \overline{x}\sum_{i=1}^m{y_i}}{\sum_{i=1}^m{x_i^2} - \overline{x}\sum_{i=1}^m{x_i}} \\
&=\frac{\sum_{i=1}^m{y_i(x_i-\overline{x})}}{\sum_{i=1}^m{x_i^2}-\frac{1}{m}(\sum_{i=1}^mx_i)^2}
\end{aligned}$$


更一般的情况是，样本有 $d$ 个属性，我们可以改写为向量和矩阵的形式，
$$\begin{aligned}
\widehat{w}&=(w;b), \\
X&=\begin{pmatrix}
x_{11} & x{12} & ... & x_{1d} & 1 \\
x_{21} & x{22} & ... & x_{2d} & 1 \\
... & ... & ... & ... & ... \\
x_{m1} & x{m2} & ... & x_{md} & 1
\end{pmatrix},\\
y&=(y_1;y_2;...;y_m)
\end{aligned}$$

得到
$$\widehat{w}^* = \argmin_{\widehat{w}}(y-X\widehat{w})^T(y-X\widehat{w})$$

求导得
$$\frac{\partial (y-X\widehat{w})^T(y-X\widehat{w})}{\partial \widehat{w}} = 2X^T(X\widehat{w}-y)$$

这个时候的情况比单属性要复杂，不能直接令上式等于零求得闭式解（因为若直接等于零求 $\widehat{w}^*$，涉及到逆矩阵，然而不是所有的矩阵都存在逆矩阵，只有满秩方阵才有逆矩阵

>满秩矩阵：设 $A$ 是 $n$ 阶矩阵，若 $r(A)=n$，则称 $A$ 为满秩矩阵。当 $A$ 同时行满秩和列满秩时，称为 $n$ 阶满秩方阵
>>矩阵的秩：对矩阵进行初等行变换化为阶梯型矩阵，其中非零行的个数成为该矩阵的秩，记为 $r(A)$


回到上一个公式，一般情况下 $X^TX$ 不是满秩矩阵，此时可以解出多个 $\widehat{w}$，具体选择哪一个解作为输出取决于学习算法的归纳偏好。此时可以引入正则化项

线性模型有丰富的变化。例如我们希望线性模型的预测值逼近真实标记 $y$ 时，得到了线性回归模型 $y=w^Tx+b$

我们也可以让线性模型逼近 $y$ 的衍生物，例如将它的对数作为逼近的目标，得到对数线性回归 $\ln{y}=w^Tx+b$，它实际上是在让 $e^{w^Tx+b}$ 逼近 $y$

更一般地，考虑单调可微函数 $g(\cdot)$（连续且充分光滑），令 $y=g^{-1}(w^Tx+b)$，得到广义线性模型，其中 $g(\cdot)$ 称为联系函数

<br>

## 对数几率回归
之前讲的是使用线性模型进行回归学习，但假如要做分类任务呢？考虑二分类任务，我们可以找到一个合适的广义线性模型，将线性回归的值和真实标签联系起来。举个例子，我们想将线性回归模型产生的实值转换为标签 $0$ 或 $1$

第一反应是使用单位越阶函数
$$y=\begin{cases}
0,& z<0 \\
0.5,& z=0 \\
1,& z>0
\end{cases}$$

但是该函数不连续，不能作为联系函数。常用的替代函数为对数几率函数 (logistic function)
$$y=\frac{1}{1+e^(-z)}$$

将对数几率函数代入，得到
$$y=\frac{1}{1+e^{-(w^Tx+b)}}$$
$$\ln{\frac{y}{1-y}}=w^Tx+b$$


若将 $y$ 视为类后验概率估计 $p(y=1 | x)$，则上式可重写为
$$\ln{\frac{p(y=1 | x)}{p(y=0 | x)}} = w^Tx+b$$
>后验概率：在观测到结果之后，对参数的估计

显然有
$$\begin{aligned}
p(y=1|x)&=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}, \\
p(y=0|x)&=\frac{1}{1+e^{w^Tx+b}}
\end{aligned}$$

此时可以通过极大似然法来估计 $w$ 和 $b$，对数几率回归模型最大化对数似然
$$l(w,b) = \sum_{i=1}^m\ln{p(y_i | x_i;w,b)}$$

>极大似然法：给定输出 $x$ 时，关于参数 $\theta$ 的似然函数 $l(\theta | x)$ 在数值上等于给定参数 $\theta$ 后变量 $X$ 的概率 $P(X=x | \theta)$

不妨令 $\beta = (w;b)$，$\widehat{x} = (x;1)$，$p_1(\widehat{x};\beta) = p(y=1 | \widehat{x};\beta)$，$p_0(\widehat{x};\beta) = p(y=0 | \widehat{x};\beta)$，则似然项可重写为以下式子（因为 $y_i$ 不是等于 $0$ 就是等于 $1$）
$$p(y_i | x_i;w,b) = y_ip_1(\widehat{x};\beta) + (1-y_i)p_0(\widehat{x};\beta)$$

代入得
$$\begin{aligned}
l(\beta) &= \sum_{i=1}^m\ln(y_ip_1(\widehat{x};\beta) + (1-y_i)p_0(\widehat{x};\beta)) \\
&=\sum_{i=1}^m\ln(\frac{e^{\beta^T\widehat{x_i}}y_i + 1 - y_i}{1+e^{\beta^T\widehat{x_i}}}) \\
&= \sum_{i=1}^m(\ln(e^{\beta^T\widehat{x_i}}y_i + 1 - y_i) - \ln(1+e^{\beta^T\widehat{x_i}}))
\end{aligned}$$

由于 $y_i=0 or 1$，有
$$l(\beta)=\begin{cases}
\sum_{i=1}^m(-\ln(1+e^{\beta^T\widehat{x_i}})) & y_i=0, \\
\sum_{i=1}^m(\beta^T\widehat{x_i} - \ln(1+e^{\beta^T\widehat{x_i}})) & y_i=1
\end{cases}$$

合起来可以写成
$$
l(\beta)=\sum_{i=1}^m(y_i\beta^T\widehat{x_i} - \ln(1+e^{\beta^T\widehat{x_i}}))
$$
我们想要最大化上式似然函数，其实就是最小化似然函数的相反数。根据凸优化理论，梯度下降法、牛顿法等都可以求得其最优解
>牛顿法：在原函数的某一点处用导数近似原函数，然后用这个导数的零点作为原函数的下一个迭代点

<br>

## 线性判别分析
线性判别分析 (LDA) 是一种经典的线性学习方法，在二分类上因为最早由 Fisher 提出，因此也叫 Fisher 判别分析
>严格来说 LDA 和 Fisher 判别法有不同，前者假设了各类样本的协方差矩阵相同且满秩
>>协方差：在概率论和统计学中用于衡量两个变量的总体误差。
>>$$\begin{aligned}
COV(X,Y) &= E[(X-E[X])(Y-E[Y])] \\
&=E[XY] - E[X]E[Y]
\end{aligned}$$
>>协方差矩阵：每个元素是各个向量元素之间的协方差

LDA 的思想：将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；对于新鲜样本，同样将其投影到该条直线上，再根据投影点的位置来确定新样本的类别
>为什么书上画的 $w$ 为过原点的直线：我们所关心的仅仅是这些点到一条直线投下来的影子点之间的距离，所以直线可以沿着投影方向随意平移，这并不影响它的影子点之间的距离。因此直线经过垂直方向平移后，肯定可以过原点

对于给定的数据集 $D={(x_i,y_i)}_{i=1}^m, y_i \in{0,1}$，令 $X_i$、$\mu_i$、$\sum_i$ 分别表示第 $i$ 类样本的集合、均值向量、协方差矩阵
>均值向量：样本点各维度的均值构成的向量

若将数据投影到直线 $w$ 上，则两类样本的中心在直线上的投影分别为 $w^T\mu_0$ 和 $w^T\mu_1$
>如何理解：可以将 $w$ 看作该直线方向的单位方向向量，则 $w^T\mu_i$ 可以用向量内积来理解
>$$\begin{aligned}
w^T\mu_i&=<w,\mu_i> \\
&=|w||\mu_i|\cos\theta \\
&=|\mu_i|\cos\theta
\end{aligned}$$
>经过该变换后所有的点都转换为这条直线上的点，也就是样本中心在直线上的投影

若将所有的样本点都投影到该直线上，则两类样本的协方差分别为 $w^T\sum_0w$ 和 $w^T\sum_1w$
>如何理解：以第 $0$ 类为例，其协方差为 $\frac{1}{m}\sum_{x\in X_0}(w^Tx-w^T\mu_0)^2$，其中
>$$\begin{aligned}
(w^Tx-w^T\mu_0)^2&=(w^T(x-\mu_0))^2 \\
&=w^T(x-\mu_0)(x-\mu_0)^Tw
\end{aligned}$$
>中间是样本点的协方差矩阵（缺少分母），则原协方差为
>$$\frac{1}{m}\sum_{x\in X_0}(w^T(x-\mu_0)(x-\mu_0)^Tw) = w^T\sum_0w$$

