# 第3章 线性模型

## 基本形式
线性模型：试图学得一个通过属性的线性组合来进行预测的函数，即
$$f(x) = w_1x_1 + w_2x_2 + ... + w_dx_d + b$$

<br>

## 线性回归
首先我们看一个属性只有一个的例子：

线性回归试图学得
$$f(x_i) = wx_i + b，使得 f(x_i) \approx y_i$$
此时可以通过使均方误差 (MSE) 最小化来求解 $w$ 和 $b$。均方误差就是测量值的平方和的均值，即
$$(w^*, b^*) = \argmin_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2$$
基于均方误差最小化来进行模型求解的方法称为 “最小二乘法”

因此，可以直接将上式分别对 $w$ 和 $b$ 求导，并令其等于零，得到 $w$ 和 $b$ 最优解的闭式解
>闭式解：即解析解，指由严格的公式求出来的解，对于任意独立变量，我们都可以将其代入解析式，求得正确的值<br><br>数值解：和解析解对应，是指给出一系列对应的自变量，采用数值方法求出的解。他人只能利用数值计算的结果，而不能随意给出自变量并求出计算值<br><br>通俗理解：已知授人以鱼不如授人以渔，则解析解=渔，数值解=鱼

$$\begin{aligned}
\frac{\partial E_{(w,b)}}{\partial w}&=\frac{\partial ((y_1-wx_1-x)^2 + ... + (y_m-wx_m-b)^2)}{\partial w} \\ 
&=2(y_1-wx_1-b)(-x_1) + ... + 2(y_m-wx_m-b)(-x_m) \\
&=2(w\sum_{i=1}^mx_i^2 - \sum_{i=1}^m(y_i-b)x_i), \\

\frac{\partial E_{(w,b)}}{\partial b}&=\frac{\partial ((y_1-wx_1-x)^2 + ... + (y_m-wx_m-b)^2)}{\partial b} \\
&=2(mb - \sum_{i=1}^m(y_i-w_xi))
\end{aligned}$$

令其等于零，解得

$$\begin{aligned}
b&=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i), \\
w\sum_{i=1}^m{x_i^2}&=\sum_{i=1}^m(y_i-b)x_i \\
&=\sum_{i=1}^m{y_ix_i} - \sum_{i=1}^m{bx_i},
\end{aligned}$$

由于
$$\begin{aligned}
b&=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i) \\
&=\overline{y}-w\overline{x},
\end{aligned}$$
代入得
$$\begin{aligned}
w\sum_{i=1}^m{x_i^2}&=\sum_{i=1}^m{y_ix_i} - \sum_{i=1}^m{(\overline{y}-w\overline{x})x_i} \\
&=\sum_{i=1}^m{y_ix_i} - \overline{y}\sum_{i=1}^m{x_i} + w\overline{x}\sum_{i=1}^m{x_i},
\end{aligned}$$
因此有
$$\begin{aligned}
w&=\frac{\sum_{i=1}^my_ix_i - \overline{y}\sum_{i=1}^m{x_i}}{\sum_{i=1}^m{x_i^2} - \overline{x}\sum_{i=1}^m{x_i}} \\
&=\frac{\sum_{i=1}^m{y_ix_i} - \overline{x}\sum_{i=1}^m{y_i}}{\sum_{i=1}^m{x_i^2} - \overline{x}\sum_{i=1}^m{x_i}} \\
&=\frac{\sum_{i=1}^m{y_i(x_i-\overline{x})}}{\sum_{i=1}^m{x_i^2}-\frac{1}{m}(\sum_{i=1}^mx_i)^2}
\end{aligned}$$


更一般的情况是，样本有 $d$ 个属性，我们可以改写为向量和矩阵的形式，
$$\begin{aligned}
\widehat{w}&=(w;b), \\
X&=\begin{pmatrix}
x_{11} & x{12} & ... & x_{1d} & 1 \\
x_{21} & x{22} & ... & x_{2d} & 1 \\
... & ... & ... & ... & ... \\
x_{m1} & x{m2} & ... & x_{md} & 1
\end{pmatrix},\\
y&=(y_1;y_2;...;y_m)
\end{aligned}$$

得到
$$\widehat{w}^* = \argmin_{\widehat{w}}(y-X\widehat{w})^T(y-X\widehat{w})$$

求导得
$$\frac{\partial (y-X\widehat{w})^T(y-X\widehat{w})}{\partial \widehat{w}} = 2X^T(X\widehat{w}-y)$$

这个时候的情况比单属性要复杂，不能直接令上式等于零求得闭式解（因为若直接等于零求 $\widehat{w}^*$，涉及到逆矩阵，然而不是所有的矩阵都存在逆矩阵，只有满秩方阵才有逆矩阵

>满秩矩阵：设 $A$ 是 $n$ 阶矩阵，若 $r(A)=n$，则称 $A$ 为满秩矩阵。当 $A$ 同时行满秩和列满秩时，称为 $n$ 阶满秩方阵
>>矩阵的秩：对矩阵进行初等行变换化为阶梯型矩阵，其中非零行的个数成为该矩阵的秩，记为 $r(A)$


回到上一个公式，一般情况下 $X^TX$ 不是满秩矩阵，此时可以解出多个 $\widehat{w}$，具体选择哪一个解作为输出取决于学习算法的归纳偏好。此时可以引入正则化项

线性模型有丰富的变化。例如我们希望线性模型的预测值逼近真实标记 $y$ 时，得到了线性回归模型 $y=w^Tx+b$

我们也可以让线性模型逼近 $y$ 的衍生物，例如将它的对数作为逼近的目标，得到对数线性回归 $\ln{y}=w^Tx+b$，它实际上是在让 $e^{w^Tx+b}$ 逼近 $y$

更一般地，考虑单调可微函数 $g(\cdot)$（连续且充分光滑），令 $y=g^{-1}(w^Tx+b)$，得到广义线性模型，其中 $g(\cdot)$ 称为联系函数

<br>

## 对数几率回归

