# 第3章 线性模型

## 基本形式
线性模型：试图学得一个通过属性的线性组合来进行预测的函数，即
$$f(x) = w_1x_1 + w_2x_2 + ... + w_dx_d + b$$

<br>

## 线性回归
首先我们看一个属性只有一个的例子：

线性回归试图学得
$$f(x_i) = wx_i + b，使得 f(x_i) \approx y_i$$
此时可以通过使均方误差 (MSE) 最小化来求解 $w$ 和 $b$。均方误差就是测量值的平方和的均值，即
$$(w^*, b^*) = \argmin_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2$$
基于均方误差最小化来进行模型求解的方法称为 “最小二乘法”

因此，可以直接将上式分别对 $w$ 和 $b$ 求导，并令其等于零，得到 $w$ 和 $b$ 最优解的闭式解
>闭式解：即解析解，指由严格的公式求出来的解，对于任意独立变量，我们都可以将其代入解析式，求得正确的值<br><br>数值解：和解析解对应，是指给出一系列对应的自变量，采用数值方法求出的解。他人只能利用数值计算的结果，而不能随意给出自变量并求出计算值<br><br>通俗理解：已知授人以鱼不如授人以渔，则解析解=渔，数值解=鱼

$$\begin{aligned}
\frac{\partial E_{(w,b)}}{\partial w}&=\frac{\partial ((y_1-wx_1-b)^2 + ... + (y_m-wx_m-b)^2)}{\partial w} \\ 
&=2(y_1-wx_1-b)(-x_1) + ... + 2(y_m-wx_m-b)(-x_m) \\
&=2(w\sum_{i=1}^mx_i^2 - \sum_{i=1}^m(y_i-b)x_i), \\

\frac{\partial E_{(w,b)}}{\partial b}&=\frac{\partial ((y_1-wx_1-b)^2 + ... + (y_m-wx_m-b)^2)}{\partial b} \\
&=2(mb - \sum_{i=1}^m(y_i-w_xi))
\end{aligned}$$

令其等于零，解得

$$\begin{aligned}
b&=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i), \\
w\sum_{i=1}^m{x_i^2}&=\sum_{i=1}^m(y_i-b)x_i \\
&=\sum_{i=1}^m{y_ix_i} - \sum_{i=1}^m{bx_i},
\end{aligned}$$

由于
$$\begin{aligned}
b&=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i) \\
&=\overline{y}-w\overline{x},
\end{aligned}$$
代入得
$$\begin{aligned}
w\sum_{i=1}^m{x_i^2}&=\sum_{i=1}^m{y_ix_i} - \sum_{i=1}^m{(\overline{y}-w\overline{x})x_i} \\
&=\sum_{i=1}^m{y_ix_i} - \overline{y}\sum_{i=1}^m{x_i} + w\overline{x}\sum_{i=1}^m{x_i},
\end{aligned}$$
因此有
$$\begin{aligned}
w&=\frac{\sum_{i=1}^my_ix_i - \overline{y}\sum_{i=1}^m{x_i}}{\sum_{i=1}^m{x_i^2} - \overline{x}\sum_{i=1}^m{x_i}} \\
&=\frac{\sum_{i=1}^m{y_ix_i} - \overline{x}\sum_{i=1}^m{y_i}}{\sum_{i=1}^m{x_i^2} - \overline{x}\sum_{i=1}^m{x_i}} \\
&=\frac{\sum_{i=1}^m{y_i(x_i-\overline{x})}}{\sum_{i=1}^m{x_i^2}-\frac{1}{m}(\sum_{i=1}^mx_i)^2}
\end{aligned}$$


更一般的情况是，样本有 $d$ 个属性，我们可以改写为向量和矩阵的形式，
$$\begin{aligned}
\widehat{w}&=(w;b), \\
X&=\begin{pmatrix}
x_{11} & x{12} & ... & x_{1d} & 1 \\
x_{21} & x{22} & ... & x_{2d} & 1 \\
... & ... & ... & ... & ... \\
x_{m1} & x{m2} & ... & x_{md} & 1
\end{pmatrix},\\
y&=(y_1;y_2;...;y_m)
\end{aligned}$$

得到
$$\widehat{w}^* = \argmin_{\widehat{w}}(y-X\widehat{w})^T(y-X\widehat{w})$$

求导得
$$\frac{\partial (y-X\widehat{w})^T(y-X\widehat{w})}{\partial \widehat{w}} = 2X^T(X\widehat{w}-y)$$

这个时候的情况比单属性要复杂，不能直接令上式等于零求得闭式解（因为若直接等于零求 $\widehat{w}^*$，涉及到逆矩阵，然而不是所有的矩阵都存在逆矩阵，只有满秩方阵才有逆矩阵

>满秩矩阵：设 $A$ 是 $n$ 阶矩阵，若 $r(A)=n$，则称 $A$ 为满秩矩阵。当 $A$ 同时行满秩和列满秩时，称为 $n$ 阶满秩方阵
>>矩阵的秩：对矩阵进行初等行变换化为阶梯型矩阵，其中非零行的个数成为该矩阵的秩，记为 $r(A)$


回到上一个公式，一般情况下 $X^TX$ 不是满秩矩阵，此时可以解出多个 $\widehat{w}$，具体选择哪一个解作为输出取决于学习算法的归纳偏好。此时可以引入正则化项

线性模型有丰富的变化。例如我们希望线性模型的预测值逼近真实标记 $y$ 时，得到了线性回归模型 $y=w^Tx+b$

我们也可以让线性模型逼近 $y$ 的衍生物，例如将它的对数作为逼近的目标，得到对数线性回归 $\ln{y}=w^Tx+b$，它实际上是在让 $e^{w^Tx+b}$ 逼近 $y$

更一般地，考虑单调可微函数 $g(\cdot)$（连续且充分光滑），令 $y=g^{-1}(w^Tx+b)$，得到广义线性模型，其中 $g(\cdot)$ 称为联系函数

<br>

## 对数几率回归
之前讲的是使用线性模型进行回归学习，但假如要做分类任务呢？考虑二分类任务，我们可以找到一个合适的广义线性模型，将线性回归的值和真实标签联系起来。举个例子，我们想将线性回归模型产生的实值转换为标签 $0$ 或 $1$

第一反应是使用单位越阶函数
$$y=\begin{cases}
0,& z<0 \\
0.5,& z=0 \\
1,& z>0
\end{cases}$$

但是该函数不连续，不能作为联系函数。常用的替代函数为对数几率函数 (logistic function)
$$y=\frac{1}{1+e^(-z)}$$

将对数几率函数代入，得到
$$y=\frac{1}{1+e^{-(w^Tx+b)}}$$
$$\ln{\frac{y}{1-y}}=w^Tx+b$$


若将 $y$ 视为类后验概率估计 $p(y=1 | x)$，则上式可重写为
$$\ln{\frac{p(y=1 | x)}{p(y=0 | x)}} = w^Tx+b$$
>后验概率：在观测到结果之后，对参数的估计

显然有
$$\begin{aligned}
p(y=1|x)&=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}, \\
p(y=0|x)&=\frac{1}{1+e^{w^Tx+b}}
\end{aligned}$$

此时可以通过极大似然法来估计 $w$ 和 $b$，对数几率回归模型最大化对数似然
$$l(w,b) = \sum_{i=1}^m\ln{p(y_i | x_i;w,b)}$$

>极大似然法：给定输出 $x$ 时，关于参数 $\theta$ 的似然函数 $l(\theta | x)$ 在数值上等于给定参数 $\theta$ 后变量 $X$ 的概率 $P(X=x | \theta)$

不妨令 $\beta = (w;b)$，$\widehat{x} = (x;1)$，$p_1(\widehat{x};\beta) = p(y=1 | \widehat{x};\beta)$，$p_0(\widehat{x};\beta) = p(y=0 | \widehat{x};\beta)$，则似然项可重写为以下式子（因为 $y_i$ 不是等于 $0$ 就是等于 $1$）
$$p(y_i | x_i;w,b) = y_ip_1(\widehat{x};\beta) + (1-y_i)p_0(\widehat{x};\beta)$$

代入得
$$\begin{aligned}
l(\beta) &= \sum_{i=1}^m\ln(y_ip_1(\widehat{x};\beta) + (1-y_i)p_0(\widehat{x};\beta)) \\
&=\sum_{i=1}^m\ln(\frac{e^{\beta^T\widehat{x_i}}y_i + 1 - y_i}{1+e^{\beta^T\widehat{x_i}}}) \\
&= \sum_{i=1}^m(\ln(e^{\beta^T\widehat{x_i}}y_i + 1 - y_i) - \ln(1+e^{\beta^T\widehat{x_i}}))
\end{aligned}$$

由于 $y_i=0 or 1$，有
$$l(\beta)=\begin{cases}
\sum_{i=1}^m(-\ln(1+e^{\beta^T\widehat{x_i}})) & y_i=0, \\
\sum_{i=1}^m(\beta^T\widehat{x_i} - \ln(1+e^{\beta^T\widehat{x_i}})) & y_i=1
\end{cases}$$

合起来可以写成
$$
l(\beta)=\sum_{i=1}^m(y_i\beta^T\widehat{x_i} - \ln(1+e^{\beta^T\widehat{x_i}}))
$$
我们想要最大化上式似然函数，其实就是最小化似然函数的相反数。根据凸优化理论，梯度下降法、牛顿法等都可以求得其最优解
>牛顿法：在原函数的某一点处用导数近似原函数，然后用这个导数的零点作为原函数的下一个迭代点

<br>

## 线性判别分析
线性判别分析 (LDA) 是一种经典的线性学习方法，在二分类上因为最早由 Fisher 提出，因此也叫 Fisher 判别分析
>严格来说 LDA 和 Fisher 判别法有不同，前者假设了各类样本的协方差矩阵相同且满秩
>>协方差：在概率论和统计学中用于衡量两个变量的总体误差。
>>$$\begin{aligned}
COV(X,Y) &= E[(X-E[X])(Y-E[Y])] \\
&=E[XY] - E[X]E[Y]
\end{aligned}$$
>>协方差矩阵：每个元素是各个向量元素之间的协方差

LDA 的思想：将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；对于新鲜样本，同样将其投影到该条直线上，再根据投影点的位置来确定新样本的类别
>为什么书上画的 $w$ 为过原点的直线：我们所关心的仅仅是这些点到一条直线投下来的影子点之间的距离，所以直线可以沿着投影方向随意平移，这并不影响它的影子点之间的距离。因此直线经过垂直方向平移后，肯定可以过原点

对于给定的数据集 $D={(x_i,y_i)}_{i=1}^m, y_i \in{0,1}$，令 $X_i$、$\mu_i$、$\sum_i$ 分别表示第 $i$ 类样本的集合、均值向量、协方差矩阵
>均值向量：样本点各维度的均值构成的向量

若将数据投影到直线 $w$ 上，则两类样本的中心在直线上的投影分别为 $w^T\mu_0$ 和 $w^T\mu_1$
>如何理解：可以将 $w$ 看作该直线方向的单位方向向量，则 $w^T\mu_i$ 可以用向量内积来理解
>$$\begin{aligned}
w^T\mu_i&=<w,\mu_i> \\
&=|w||\mu_i|\cos\theta \\
&=|\mu_i|\cos\theta
\end{aligned}$$
>经过该变换后所有的点都转换为这条直线上的点，也就是样本中心在直线上的投影

若将所有的样本点都投影到该直线上，则两类样本的协方差分别为 $w^T\sum_0w$ 和 $w^T\sum_1w$
>如何理解：以第 $0$ 类为例，其协方差为 $\frac{1}{m}\sum_{x\in X_0}(w^Tx-w^T\mu_0)^2$，其中
>$$\begin{aligned}
(w^Tx-w^T\mu_0)^2&=(w^T(x-\mu_0))^2 \\
&=w^T(x-\mu_0)(x-\mu_0)^Tw
\end{aligned}$$
>中间是样本点的协方差矩阵（缺少分母），则原协方差为
>$$\frac{1}{m}\sum_{x\in X_0}(w^T(x-\mu_0)(x-\mu_0)^Tw) = w^T\sum_0w$$

我们要使同类样本的投影点尽可能接近，也就是让同类样本投影点的协方差尽可能小；异类样本的投影点尽可能远离，也就是让类中心点的距离尽可能大。同时考虑可得到以下最大化目标：
$$\begin{aligned}
J &= \frac{||w^T\mu_0 - w^T\mu_1||_2^2}{w^T\sum_0w + w^T\sum_1w} \\
&= \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\sum_0+\sum_1)w}
\end{aligned}$$

定义 “类内散度矩阵”：
$$\begin{aligned}
S_w &= \sum_0 + \sum_1 \\
&= \sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T + \sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T
\end{aligned}$$

以及 “类间散度矩阵”：
$$S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T$$

则最大化目标可重写为下式，这是 LDA 最大化的目标，也是 $S_b$ 和 $S_w$ 的 “广义瑞利商”
$$
J = \frac{w^TS_bw}{w^TS_ww}
$$

可以发现，上式分子分母都是关于 $w$ 的二次项，因此其解和 $w$ 的长度无关，只与其方向有关。不妨令 $w^TS_ww = 1$，则上式等价于
$$
\min_w  -w^TS_bw \\
s.t.  w^TS_ww = 1
$$

接下来使用拉格朗日乘子法：
>拉格朗日乘子法：可用来求目标函数 $z=f(x,y)$ 在约束条件 $\varphi(x,y)=0$ 下的极值<br>不妨令 $L(x,y,\lambda) = f(x,y) + \lambda\varphi(x,y)$，可以用以下方程组求解所有的解 $(x_0,y_0,\lambda)$：
>$$\begin{cases}
f_x(x,y) + \lambda\varphi_x(x,y) = 0, \\
f_y(x,y) + \lambda\varphi_y(x,y) = 0, \\
\varphi(x,y) = 0
\end{cases}$$

我们可以写出对应的拉格朗日函数：$L(w,\lambda) = -w^TS_bw + \lambda(w^TS_ww-1)$
>矩阵求导：$\frac{\partial x^TAx}{\partial x} = (A+A^T)x$

对 $w$ 求偏导：
$$\begin{aligned}
\frac{\partial L(w,\lambda)}{\partial w} &= -\frac{\partial (w^TS_bw)}{\partial w} + \lambda \frac{\partial(w^TS_ww-1)}{\partial w} \\
&= -(S_b+S_b^T)w + \lambda(S_w+S_w^T)w \\
&= -2S_bw + 2\lambda S_ww
\end{aligned}$$

令上式等于0，得到 $S_bw = \lambda S_ww$

注意到 $S_bw$ 的方向恒为 $\mu_0 - \mu_1$，不妨令 $S_bw = \lambda (\mu_0 - \mu_1)$，代入解得 $w = S_w^{-1}(\mu_0 - \mu_1)$

在实践中通常通过对 $S_w$ 进行奇异值分解，即 $S_w = U\sum V^T$，然后再由 $V\sum^{-1}U^T = S_w^{-1}$ 得到 $S_w^{-1}$
>奇异值分解：SVD 是一个强大的工具，可以将矩阵 $A$ 分解为 $U\sum V^T$，其中 $U$ 和 $V$ 是正交矩阵，$\sum$ 是对角矩阵，对角线上元素为奇异值
>>数学理解：若将矩阵视为一种线性变换，可以将 SVD 理解为，对于任意矩阵 $A$，总能找到一组单位正交基，使得 $A$ 对其进行变换之后，得到的向量依然是正交的<br><br>
>>几何理解：$A=U\sum V^T$ 表示矩阵 $A$ 所代表的线性变换可以由更简单的旋转、伸缩变换进行合成，奇异值表示对标准正交基各个轴进行伸缩的程度<br><br>
>>求解过程：对于 $A=U\sum V^T$，可以得到
>>$$\begin{aligned}
AA^T = U\sum V^TV\sum U^T &= U\sum^2 U^T, \\
A^TA = V\sum U^TU\sum V^T &= V\sum^2 V^T
\end{aligned}$$
>>由于实对称矩阵必可以正交对角化，即 $S = QAQ^T$，其中 $Q$ 满足 $Q^T = Q^{-1}$，$A$ 为矩阵 $S$ 的特征值组成的对角矩阵，所以可以通过对 $AA^T$ 和 $A^TA$ 进行正交对角化求得 $U$ 和 $V$，进而求出 $\sum$


可以进一步将 LDA 推广到多分类任务中。假设存在 $N$ 个类，第 $i$ 类的样本数为 $m_i$

首先定义 “全局散度矩阵”，其中 $\mu$ 是所有 $m$ 个样本的均值向量
$$\begin{aligned}
S_t &= S_b + S_w \\
&= \sum_{i=1}^{m}(x_i-\mu)(x_i-\mu)^T
\end{aligned}$$

再将类内散度矩阵 $S_w$ 重新定义为每个类别的散度矩阵之和，即 $S_{w} = \sum_{i=1}^N S_{w_i}$

因此有
$$\begin{aligned}
S_b &= S_t - S_w \\
&= \sum_{i=1}^{m}(x_i-\mu)(x_i-\mu)^T - \sum_{i=1}^N \sum_{x \in X_i}(x-\mu_i)(x-\mu_i)^T \\
&= \sum_{i=1}^N(\sum_{x \in X_i}((x-\mu)(x-\mu)^T - (x-\mu_i)(x-\mu_i)^T)) \\
&= \sum_{i=1}^N(\sum_{x \in X_i}((x-\mu)(x^T-\mu^T) - (x-\mu_i)(x^T-\mu_i^T))) \\
&= \sum_{i=1}^N(\sum_{x \in X_i}(-x\mu^T-\mu x^T+\mu\mu^T+x\mu_i^T+\mu_i x^T-\mu_i \mu_i^T)) \\
&= \sum_{i=1}^N(-m_i\mu_i\mu^T -m_i\mu\mu_i^T + m_i\mu\mu^T + m_i\mu_i\mu_i^T) \\
&= \sum_{i=1}^N m_i(\mu_i-\mu)(\mu_i-\mu)^T
\end{aligned}$$

多分类的 LDA 有多种实现方法，只需要使用 $S_b$、$S_w$、$S_t$ 三者中的任意两个就行

其中有一种可以通过扩展 $\frac{w^TS_bw}{w^TS_ww}$ 得到。不妨设 $W = (w_1,w_2,...,w_{N-1}) \in R^{d \times (N-1)}$，其中 $w_i \in R^{d \times 1}$ 为 $d$ 行 $1$ 列的列向量，则有
$$\begin{cases}
tr(W^TS_bW) = \sum_{i=1}^{N-1}w_i^TS_bw_i \\
tr(W^TS_wW) = \sum_{i=1}^{N-1}w_i^TS_ww_i
\end{cases}$$

因此之前得到的公式可以扩展为
$$
\max_W \frac{\sum_{i=1}^{N-1}w_i^TS_bw_i}{\sum_{i=1}^{N-1}w_i^TS_ww_i}
=\max_W \frac{tr(W^TS_bW)}{tr(W^TS_wW)}
$$

和二分类的情景一样，我们也可以规定分母为 1，那么上述公式等价于
$$
\min_W -tr(W^TS_bW) \\
s.t. tr(W^TS_wW)=1
$$

得到拉格朗日函数 $L(W,\lambda) = -tr(W^TS_bW) + \lambda(tr(W^TS_wW)-1)$，因此
$$\begin{aligned}
\frac{\partial L(W,\lambda)}{\partial W} &= -(S_b+S_b^T)W + \lambda(S_w+S_w^T)W \\
&= -2S_bW + 2\lambda S_wW
\end{aligned}$$

令上式等于零，解得 $S_bW = \lambda S_wW$

若将 $W$ 视为一个投影矩阵，则多分类 LDA 将样本投影到 $N-1$ 维空间，因此 LDA 也常被视为一种经典的监督降维技术

<br>

## 多分类学习

面对多分类问题，有的时候可以像上面提到的 LDA，直接将二分类学习方法推广到多分类。但更多情况下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题的。其基本思路是 “拆解法”，即将多分类任务拆解为若干个二分类任务。这里的关键是如何对多分类任务进行拆分，以及如何对多个分类器进行集成以获得最终的多分类结果。

经典的拆解策略有三种：（1）一对一 (OvO)；（2）一对其余 (OvR)；（3）多对多 (MvM)

给定数据集 $D = \{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}, y_i \in \{C_1,C_2,...,C_N\}$，以此为例我们来分别看看以上三种策略的具体操作

OvO：将这 $N$ 个类别两两配对，从而产生 $N(N-1)/2$ 个二分类任务

OvR：每次将一个类作为正类、所有其它类为反类来训练 $N$ 个分类器；在测试时如果仅有一个分类器预测为正类，则该标签作为最终分类结果；如果有多个分类器预测为正类，则根据各分类器的置信度进行选择

MvM：每次将若干个类作为正类，其余若干个类作为反类。在这里简单介绍一种最常用的 MvM 技术：纠错输出码 (ECOC)。ECOC 工作分为两步：

（1）编码：对 $N$ 个类别做 $M$ 次划分，每次将一部分类别划为正类，另一部分划为反类，从而形成一个二分类训练集，包含 $M$ 个训练集，可以训练出 $M$ 个分类器

（2）解码：用上述 $M$ 个分类器对测试样本进行预测，这些预测结果组成一个编码，将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终结果（可以使用海明距离或欧氏距离）。包含二元 ECOC 码和三元 ECOC 码

这种方法和计算机网络中学到的纠错码类似

<br>

## 类别不平衡问题

假如训练数据中，正例反例的数目差别较大，则会对学习造成较大困扰。类别不平衡问题就是指分类任务中不同类别的训练样例数目差别很大的情况。

从线性分类器的角度来说，我们对样本 $x$ 进行分类时，其实是用预测出的 $y$ 值与一个阈值进行比较，即分类器的决策逻辑为，如果 $\frac{y}{1-y} > 1$ 则预测为正例

但是，当训练集中正反例的数目不同时，不妨用 $m^+$ 表示正例的数目，$m^-$ 表示反例的数目，则观测几率为 $\frac{m^+}{m^-}$。由于我们通常假设训练集真实样本总体的无偏采样，则观测几率代表了真实几率，那么只需要分类器的预测几率高于观测几率，就应判定为正例，即如果 $\frac{y}{1-y} > \frac{m^+}{m^-}$ 则预测为正例

我们想要分类器在做决策时，执行的实际是上面那个式子，只需要令
$$
\frac{y'}{1-y'} = \frac{y}{1-y} \times \frac{m^-}{m^+}
$$
这是类别不平衡学习的一个基本策略——再放缩

但是实际操作确很难开展，因为 “训练集是真实样本总体的无偏采样” 这个假设往往不成立。现在技术大致有三种做法：

（1）对反类样例进行欠采样，即除去一些反例使得正、反样例数目接近。注意若随机丢弃反例，可能会丢失一些重要信息。其代表算法 EasyEnsemble 则是利用集成学习，将反例划分为若干个集合供不同学习器使用，这样每一个学习器都进行了欠采样，但是总体上没有丢失重要信息

（2）对正类样例进行过采样，即增加一些正例使得正、反样例数目接近。注意不能简单地对初始正例样本进行重复采样，否则会导致过拟合。其代表算法 SMOTE 是通过对训练集里的正例进行插值来产生额外的正例

（3）直接基于原始训练集学习，但是在用训练好的分类器测试时，将 $\frac{y'}{1-y'} = \frac{y}{1-y} \times \frac{m^-}{m^+}$ 嵌入到其决策过程中，称为 “阈值移动”
